# 机器学习

---



# 定义

A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its perfomance on T, as measured by P, improves with experience E. ——Tom Mitchell(1988)

计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过P测定在T上的表现因经验E而提高。



# Symbols

> - m ==> Number of training examples
> - x's ==> "input" variable / features
> - y's ==> "output" variable / "target" variable
> - (x,y) ==> one single training example
> - (x^i^,y^i^) ==> i^th^ training example
> - x^i^~j~ ==>value of feature j in i^th^ training example
> - :=  ==> assignment
> - =  ==> truth assertion
> - h(θ,x) ==> hypothesis function



# 代价函数(平方误差函数)

![image-20230104182937858](E:\Study Notes\Pictures\image-20230104182937858.png)

![image-20230104183842494](E:\Study Notes\Pictures\image-20230104183842494.png)

![image-20230104184743427](E:\Study Notes\Pictures\image-20230104184743427.png) 



# Batch梯度下降算法

Batch梯度下降算法可以应用于最小化一般函数J(θ)

![image-20230104193356733](E:\Study Notes\Pictures\image-20230104193356733.png)

![image-20230104192915391](E:\Study Notes\Pictures\image-20230104192915391.png)

> - α是被称为学习率的数字，用来控制梯度下降的步长，即更新参数的幅度，α越大梯度下降越迅速
> - 重复进行该步骤直到收敛
> - 对θ~0~和θ~1~的更新需要同时进行



## 特征放缩 

多个特征使用梯度下降算法时，如果能确保这些特征的取值都处于一个相近的范围，则梯度下降算法能更快地收敛

更一般地说，当我们执行特征缩放时，我们通常的目标是将特征的取值约束到-1≤x~i~≤1的范围内，但这个范围不是绝对的

除了将特征除以最大值以外，在特征缩放中有时我们会进行均值归一化的工作：

![image-20230105142008520](E:\Study Notes\Pictures\image-20230105142008520.png)

> μ~1~是训练集中特诊x~1~的平均值
>
> s~1~是该特征值的范围，即最大值减去最小值，也可以将s~1~设置为变量的标准差



## 学习率

通常我们会画出代价函数随迭代步数增加的变化曲线来判断梯度下降算法是否已经收敛，也可以进行一些自动收敛测试

![image-20230105150746024](E:\Study Notes\Pictures\image-20230105150746024.png)

![image-20230105151116253](E:\Study Notes\Pictures\image-20230105151116253.png)





# 正规方程算法

在最小化代价函数J(θ)时，对于某些模型可以使用正规方程算法进行最小值求解，这种方法不需要多次迭代来收敛到全局最小值，基本上只需一步就可以得到最优值，且不需要进行特征缩放，在处理的特征量不是特别大时具有较好的效率。

正规方程算法并不具有普适性，比如对线性回归算法适用而对分类算法并不适用

![image-20230111165040246](E:\Study Notes\Pictures\image-20230111165040246.png)

![image-20230111164553396](E:\Study Notes\Pictures\image-20230111164553396.png)

![image-20230111165401857](E:\Study Notes\Pictures\image-20230111165401857.png)

## 矩阵不可逆

如果X^T^X是奇异矩阵或不可逆，则无法进行逆运算

![image-20230111170929859](E:\Study Notes\Pictures\image-20230111170812765.png)





# 正则化

## 过拟合

欠拟合：算法具有高偏差，无法较好地拟合数据集

过拟合：算法具有高方差，即拟合一个高阶多项式时，假设函数h(x)能拟合几乎所有的数据，这时可能面临可能的函数太过庞大、变量太多的问题，我们没有足够的数据来约束它来获得一个好的假设函数，这个函数会非常拟合已有的数据集，但是无法泛化到新的样本中

![image-20230116133834268]( Pictures/image-20230116133834268.png)

解决方法：

![image-20230116135126459]( Pictures/image-20230116135126459.png)



## 代价函数

通过在代价函数中增加惩罚项弱化特征对拟合模型的影响 ，更好地去拟合数据集且使参数都尽可能小，以此来简化预测函数，避免出现过拟合的情况

![image-20230117141425771]( Pictures/image-20230117141425771.png)

> 正则化参数λ的作用是控制两个不同目标间的取舍，即控制这两个目标之间的平衡关系：
>
> - 与目标函数的第一项有关，即更好地拟合数据集
> - 与目标函数的第二项有关，即保持参数尽量地小，与正则化目标和正则化项有关





# 监督学习(Supervised Learning)

为算法提供包含”正确答案“的数据集



## 线性回归算法(linear regression)

为算法提供包含”正确答案“的数据集，回归是指根据已有的数据集预测一个具体的数值输出，输出结果不是离散的

### 单变量线性回归

![image-20230102173138286](E:\Study Notes\Pictures\单变量线性回归)

![image-20230104195815441](E:\Study Notes\Pictures\image-20230104195815441.png)



### 多变量线性回归

![image-20230105134420771](E:\Study Notes\Pictures\image-20230105134420771.png)

![image-20230105135024884](E:\Study Notes\Pictures\image-20230105135024884.png)



### 代价函数正则化

#### 梯度下降

![image-20230117170248279]( Pictures/image-20230117170248279.png)

#### 正规方程



![image-20230117170900260]( Pictures/image-20230117170900260.png)

不可逆问题

数学上可以证明，正则化时不会出现不可逆的情况

![image-20230117171153739]( Pictures/image-20230117171153739.png)





## 分类算法

预测离散值的输出，如预测结果只有0/1(负类/正类)



### logistic回归算法

用于预测结果y为离散值0/1的情况下，算法的输出或者说预测值一直介于0和1之间

![image-20230113153223533]( Pictures/image-20230113153223533.png)

![image-20230113153241487]( Pictures/image-20230113153241487.png)



#### 决策界限

决策边界不是训练集的属性，而是假设本身及其参数的属性，只要给定了参数向量θ那么就能确定决策边界，而不是用训练集来定义决策边界，我们用训练集来拟合参数θ

![image-20230113153943303]( Pictures/image-20230113153943303.png)



#### 参数拟合

由于g()的凸优化问题，所选的代价函数与平方误差函数有所不同

![image-20230113160934718]( Pictures/image-20230113160934718.png)

![image-20230113161159643]( Pictures/image-20230113161159643.png)



#### 简化代价函数与梯度下降

特征放缩同样适用与logistic回归

![image-20230113165135707]( Pictures/image-20230113165135707.png)



#### 高级优化

- 共轭梯度法
- BFGS
- L-BFGS

这些优化算法通常不需要手动选择学习率，这些算法有被称为线搜索算法的智能内循环，它可以自动尝试不同的学习率并自动选择一个好的学习率，甚至可以为每次迭代选择不同的学习速率

通常这些算法的收敛会远远快于梯度下降



#### 正则化

梯度下降

![image-20230117171751764](Pictures/image-20230117171751764.png)



# 无监督学习(Unsupervised Learning)



## 聚类算法

对无标签的数据集进行分簇



## 鸡尾酒会算法

分离叠加在一起的两个音频源

![image-20230102163501414](E:\Study Notes\Pictures\鸡尾酒会算法.png)



